title: 安装 Spark1.5.2 与 Hadoop2.6.2
date: 2016-02-27 17:02:21
update: 2020-03-03
categories: data
tags:

- spark
- hadoop

---

**_适用于 Hadoop 2.9_**

# build spark 1.5.2

```
./dev/change-scala-version.sh 2.11
./make-distribution.sh --name hadoop2.6-scala2.11 --tgz -Phadoop-2.6 -Pyarn -Phive -Phive-thriftserver -Dscala-2.11
```

1.6+可以添加：`-Psparkr`以支持 Spark R。

## 创建用户和组

```
sudo addgroup scdata
sudo adduser --ingroup scdata scdata
```

## SSH 无密码登录节点

```
scdata$ cd ~/.ssh                      # 如果该目录不存在，则创建：mkdir ~/.ssh
scdata$ ssh-keygen -t rsa              # 一直回车即可，生成的密码保存在 .ssh/id_rsa
```

**Master**节点需要能无密码登录本机，这一步还需要执行：

```
scdata$ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
```

完成需要将公匙传到各**Slave**节点上：

```
scdata$ scp ~/.ssh/authorized_keys scdata@salver1:~/.ssh/
scdata$ scp ~/.ssh/authorized_keys scdata@salver2:~/.ssh/
```

现在就可以直接使用`ssh hostname`免密码登录到各节点上了。

## 安装 Hadoop

```
mkdir -p /usr/app/hadoop/data
sudo chown -R scdata:scdata /usr/app/hadoop
su - scdata
scdata$ wget -c http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.6.2/hadoop-2.6.2.tar.gz
scdata$ tar zxf hadoop-2.6.2.tar.gz
```

## Hadoop 伪分布式配置

Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode。

（注：若系统 **JAVA** 比较复杂可以在 `$HADOOP_HOME/etc/hadoop/hadoop-env.sh` 文件指定 **JAVA_HOME**）

Hadoop 的配置文件位于`/usr/local/hadoop/etc/hadoop/`中，伪分布式需要修改 2 个配置文件`core-site.xml`和 `hdfs-site.xml`。Hadoop 的配置文件是 xml 格式，每个配置以声明 property 的 name 和 value 的方式来实现。

修改配置文件`core-site.xml` (vim /usr/local/hadoop/etc/hadoop/core-site.xml)，将当中的：

```xml
<configuration>
</configuration>
```

修改为如下配置：

```xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/app/hadoop/data/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://192.168.31.101:9000</value>
    </property>
</configuration>
```

（注：IP 和 PORT 设置为想要的）

同样的，修改配置文件`hdfs-site.xml`：

```
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/app/hadoop/data/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/app/hadoop/data/dfs/data</value>
    </property>
</configuration>
```

**关于 Hadoop 配置项的说明**

虽然只需要配置`fs.defaultFS`和`dfs.replication`即可运行（官方教程），但不配置`hadoop.tmp.dir`参数，刚默认使用的临时目录为`/tmp/hadoop-hadoop`。这个目录在系统重启后可能会被清理掉，导致必需重新执行`format`才行。所以最好指定`hadoop.tmp.dir`目录，同时也指定`dfs.namenode.dir`和`dfs.datanode.data.dir`。

配置完成后，执行`namenode`格式化：

```
scdata$ ./bin/hdfs namenode -format
```

成功会看到终端输出提示。倒数第 5 行为：`util.ExitUtil: Exiting with status 0`时表示成功，否则是出错。若出现错误，请查检之前步骤是否正确。

```shell
15/12/08 11:43:29 INFO util.ExitUtil: Exiting with status 0
15/12/08 11:43:29 INFO namenode.NameNode: SHUTDOWN_MSG:
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at sc-007/127.0.1.1
************************************************************/
```

接着开启**NameNode**和**DataNode**守护进程：

```
scdata$ ./sbin/start-dfs.sh
```

（注：若出现 SSH 和登录提示，输入`yes`和按要求输入即可）

启动完后可以使用`jps`命令来判断是否启动成功：

```
scdata$ jps
25889 DataNode
26070 SecondaryNameNode
28969 Jps
25743 NameNode
```

成功启动后，可以访问 Web 界面：[http://localhost:50070](http://localhost:50070)来查看**Hadoop**的信息。

## 导入数据到**Hadoop**

要在 HDFS 上使用数据，需要先创建用户目录：

```
scdata$ ./bin/hdfs dfs -mkdir -p /user/hadoop/input
```

接着将 etc/hadoop 中的文件作为输入文件复制到分布式文件系统中，即将 /usr/local/hadoop/etc/hadoop 复制到分布式文件系统中的 /user/hadoop/input 中:

```
scdata$ ./bin/hdfs dfs -put etc/hadoop/*.xml /user/hadoop/input/
scdata$ ./bin/hdfs dfs -ls /user/hadoop/input
```

## 配置集群/分布式环境

### 设置 IP 和 hostname

**（注意：在集群/分布式环境下，设置`/etc/hosts`时`127.0.0.1`（回还地址 IP）不能指向 hostname，而只应该使外网 IP 指向 hostname）**

**Hadoop**集群/分布式环境需要修改**etc/hadoop**中的 5 个配置文件：`slaves`、`core-site.xml`、`hdfs-site.xml`、`mapred-site.xml`、`yarn-site.xml`。

### **slave**

编辑`etc/hadoop/slave`文件，删除**localhost**，把所有**Slave**的主机名写上，每行一个。

### **core-site.xml**

编辑`etc/hadoop/core-site.xml`文件如下：

```
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/app/hadoop/data/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://Master:9000</value>
    </property>
</configuration>
```

### **hdfs-site.xml**

编辑`etc/hadoop/hdfs-site.xml`文件如下：

```
<configuration>
<property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>Master:50090</value>
</property>
<property>
    <name>dfs.namenode.name.dir</name>
    <value>file:/usr/app/hadoop/data/dfs/name</value>
</property>
<property>
    <name>dfs.datanode.data.dir</name>
    <value>file:/usr/app/hadoop/data/dfs/data</value>
</property>
<property>
    <name>dfs.replication</name>
    <value>1</value>
</property>
</configuration>
```

因为只有一个**Slave**，所以这里**dfs.replication**的值设为 1。

### **mapred-site.xml**

文件**mapred-site.xml**不存在，需要从模板中复制一份，然后修改配置文件如下：

```
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>
```

### **yarn-site.xml**

编辑`etc/hadoop/yarn-site.xml`文件如下：

```
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>Master</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>
```

配置好后，将 Master 上的 Hadoop 文件复制到各个节点上(虽然直接采用 scp 复制也可以正确运行，但会有所不同，如符号链接 scp 过去后就有点不一样了。所以先打包再复制比较稳妥)。因为之前有跑过伪分布式模式，建议在切换到集群模式前先删除之前的临时文件。

```
scdata$ tar zcf hadoop-2.6.tar.gz hadoop-2.6
scp hadoop-2.6.tar.gz scdata@Slave:/usr/app/hadoop/
```

（注：把 hadoop 传到每个节点并解压，设置好目录权限）

然后在**Master 节点**上启动**Hadoop**：

```
scdata$ ./bin/hdfs namenode -format                # 首次运行需要执行初始化，后面不再需要
scdata$ ./sbin/start-dfs.sh
scdata$ ./sbin/start-yarn.sh
```

在**Master**上使用`jps`命令可以查看各节点启动的进程

```
scdata$ jps
28147 Jps
27876 ResourceManager
27493 NameNode
27710 SecondaryNameNode
```

在**Slave**上使用`jps`也可以看到**Master**已经自动启动了**Slave**上的`DataNode`和`NodeManager`服务。另外也可以在**Master**节点上通过命令：`bin/hdfs dfsadmin -report`：

```
$ ./bin/hdfs dfsadmin -report
Configured Capacity: 950348005376 (885.08 GB)
Present Capacity: 235839873024 (219.64 GB)
DFS Remaining: 235839848448 (219.64 GB)
DFS Used: 24576 (24 KB)
DFS Used%: 0.00%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0

-------------------------------------------------
Live datanodes (1):

Name: 192.168.31.121:50010 (Slave)
Hostname: Slave
Decommission Status : Normal
Configured Capacity: 950348005376 (885.08 GB)
DFS Used: 24576 (24 KB)
Non DFS Used: 714508132352 (665.44 GB)
DFS Remaining: 235839848448 (219.64 GB)
DFS Used%: 0.00%
DFS Remaining%: 24.82%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Tue Dec 08 16:43:28 CST 2015
```
